{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e382d238",
   "metadata": {},
   "source": [
    "# An on-line movie recommending service using Spark & Flask - Building the recommender"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f67701d",
   "metadata": {},
   "source": [
    "## Getting and processing the data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4bad0b2",
   "metadata": {},
   "source": [
    "The list of task we can pre-compute includes:  \n",
    "\n",
    "- Loading and parsing the dataset. Persisting the resulting RDD for later use.  \n",
    "- Building the recommender model using the complete dataset. Persist the dataset for later use.  \n",
    "\n",
    "This notebook explains the first of these tasks.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84ec15b9",
   "metadata": {},
   "source": [
    "### File download"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a7b8932c",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "complete_dataset_url = 'http://files.grouplens.org/datasets/movielens/ml-latest.zip'\n",
    "small_dataset_url = 'http://files.grouplens.org/datasets/movielens/ml-latest-small.zip'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "434b1d32",
   "metadata": {},
   "source": [
    "Define download locations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "08c56796",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "datasets_path = os.path.join('..', 'datasets')\n",
    "\n",
    "complete_dataset_path = os.path.join(datasets_path, 'ml-latest.zip')\n",
    "small_dataset_path = os.path.join(datasets_path, 'ml-latest-small.zip')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e57a6e8",
   "metadata": {},
   "source": [
    "Now we can proceed with both downloads."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c6d2fb54",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "import urllib.request\n",
    "\n",
    "small_f = urllib.request.urlretrieve (small_dataset_url, small_dataset_path)\n",
    "complete_f = urllib.request.urlretrieve (complete_dataset_url, complete_dataset_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0244523",
   "metadata": {},
   "source": [
    "Both of them are zip files containing a folder with ratings, movies, etc. We need to extract them into its individual folders so we can use each file later on.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6914c77b",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "import zipfile\n",
    "\n",
    "with zipfile.ZipFile(small_dataset_path, \"r\") as z:\n",
    "    z.extractall(datasets_path)\n",
    "\n",
    "with zipfile.ZipFile(complete_dataset_path, \"r\") as z:\n",
    "    z.extractall(datasets_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95a65128",
   "metadata": {},
   "source": [
    "### Loading and parsing datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0485107d",
   "metadata": {},
   "source": [
    "Load the raw ratings data. I need to filter out the header, included in each file.    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c81b631c",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark import SparkConf, SparkContext\n",
    "\n",
    "import os\n",
    "\n",
    "conf = SparkConf().setAppName(\"RecommendationApp\").setMaster(\"local[*]\")\n",
    "\n",
    "sc = SparkContext(conf=conf)\n",
    "\n",
    "small_ratings_file = os.path.join(datasets_path, 'ml-latest-small', 'ratings.csv')\n",
    "\n",
    "small_ratings_raw_data = sc.textFile(small_ratings_file)\n",
    "small_ratings_raw_data_header = small_ratings_raw_data.take(1)[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a074d6c",
   "metadata": {},
   "source": [
    "Now we can parse the raw data into a new RDD.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "fb3b746a",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "small_ratings_data = small_ratings_raw_data.filter(lambda line: line!=small_ratings_raw_data_header)\\\n",
    "    .map(lambda line: line.split(\",\")).map(lambda tokens: (tokens[0],tokens[1],tokens[2])).cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df15cd33",
   "metadata": {},
   "source": [
    "For illustrative purposes, we can take the first few lines of our RDD to see the result. In the final script we don't call any Spark action (e.g. `take`) until needed, since they trigger actual computations in the cluster.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b5d004bb",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('1', '1', '4.0'), ('1', '3', '4.0'), ('1', '6', '4.0')]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "small_ratings_data.take(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "733a7fb6",
   "metadata": {},
   "source": [
    "I proceed in a similar way with the `movies.csv` file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f5db951b",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('1', 'Toy Story (1995)'),\n",
       " ('2', 'Jumanji (1995)'),\n",
       " ('3', 'Grumpier Old Men (1995)')]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "small_movies_file = os.path.join(datasets_path, 'ml-latest-small', 'movies.csv')\n",
    "\n",
    "small_movies_raw_data = sc.textFile(small_movies_file)\n",
    "small_movies_raw_data_header = small_movies_raw_data.take(1)[0]\n",
    "\n",
    "small_movies_data = small_movies_raw_data.filter(lambda line: line!=small_movies_raw_data_header)\\\n",
    "    .map(lambda line: line.split(\",\")).map(lambda tokens: (tokens[0],tokens[1])).cache()\n",
    "    \n",
    "small_movies_data.take(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5555d08",
   "metadata": {},
   "source": [
    "## Selecting ALS parameters using the small dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91890372",
   "metadata": {},
   "source": [
    "In order to determine the best ALS parameters, I will use the small dataset. We need first to split it into train, validation, and test datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6084ef3e",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "training_RDD, validation_RDD, test_RDD = small_ratings_data.randomSplit([6, 2, 2], seed=0)\n",
    "validation_for_predict_RDD = validation_RDD.map(lambda x: (x[0], x[1]))\n",
    "test_for_predict_RDD = test_RDD.map(lambda x: (x[0], x[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b941fed",
   "metadata": {},
   "source": [
    "Now we can proceed with the training phase. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "3acb5797",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For rank %s the RMSE is %s 4 0.9136647249990393\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For rank %s the RMSE is %s 8 0.9226236040056827\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 539:================================================>      (16 + 2) / 18]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For rank %s the RMSE is %s 12 0.9188432735769673\n",
      "The best model was trained with rank %s 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.mllib.recommendation import ALS\n",
    "import math\n",
    "\n",
    "seed = 5\n",
    "iterations = 10\n",
    "regularization_parameter = 0.1\n",
    "ranks = [4, 8, 12]\n",
    "errors = [0, 0, 0]\n",
    "err = 0\n",
    "tolerance = 0.02\n",
    "\n",
    "min_error = float('inf')\n",
    "best_rank = -1\n",
    "best_iteration = -1\n",
    "for rank in ranks:\n",
    "    model = ALS.train(training_RDD, rank, seed=seed, iterations=iterations,\n",
    "                      lambda_=regularization_parameter)\n",
    "    predictions = model.predictAll(validation_for_predict_RDD).map(lambda r: ((r[0], r[1]), r[2]))\n",
    "    rates_and_preds = validation_RDD.map(lambda r: ((int(r[0]), int(r[1])), float(r[2]))).join(predictions)\n",
    "    error = math.sqrt(rates_and_preds.map(lambda r: (r[1][0] - r[1][1])**2).mean())\n",
    "    errors[err] = error\n",
    "    err += 1\n",
    "    print ('For rank %s the RMSE is %s', rank, error)\n",
    "    if error < min_error:\n",
    "        min_error = error\n",
    "        best_rank = rank\n",
    "\n",
    "print ('The best model was trained with rank %s', best_rank)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63797b59",
   "metadata": {},
   "source": [
    "But let's explain this a little bit. First, let's have a look at how our predictions look.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ad78c61d",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[((599, 1840), 2.81069881559431),\n",
       " ((11, 1840), 3.200947771281338),\n",
       " ((541, 464), 1.98838227782107)]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions.take(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93dcad4e",
   "metadata": {},
   "source": [
    "Basically we have the UserID, the MovieID, and the Rating, as we have in our ratings dataset. In this case the predictions third element, the rating for that movie and user, is the predicted by our ALS model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec510ad6",
   "metadata": {},
   "source": [
    "Then we join these with our validation data (the one that includes ratings) and the result looks as follows:  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "e4c20385",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[((1, 3441), (5.0, 3.1102836692112596)),\n",
       " ((2, 48516), (4.0, 4.031505125791396)),\n",
       " ((2, 80906), (5.0, 3.4547736654339793))]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rates_and_preds.take(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70671344",
   "metadata": {},
   "source": [
    "To that, we apply a squared difference and the we use the `mean()` action to get the MSE and apply `sqrt`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60589c9a",
   "metadata": {},
   "source": [
    "Finally we test the selected model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "df7fc0cd",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 771:==========================================>            (14 + 4) / 18]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For testing data the RMSE is %s 0.908279077289237\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "model = ALS.train(training_RDD, best_rank, seed=seed, iterations=iterations,\n",
    "                      lambda_=regularization_parameter)\n",
    "predictions = model.predictAll(test_for_predict_RDD).map(lambda r: ((r[0], r[1]), r[2]))\n",
    "rates_and_preds = test_RDD.map(lambda r: ((int(r[0]), int(r[1])), float(r[2]))).join(predictions)\n",
    "error = math.sqrt(rates_and_preds.map(lambda r: (r[1][0] - r[1][1])**2).mean())\n",
    "    \n",
    "print ('For testing data the RMSE is %s', error)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b09574d8",
   "metadata": {},
   "source": [
    "## Using the complete dataset to build the final model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e02d60d4",
   "metadata": {},
   "source": [
    "In order to build our recommender model, we will use the complete dataset. Therefore, we need to process it the same way we did with the small dataset.   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "b469e40c",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 778:=============================================>         (23 + 5) / 28]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are {} recommendations in the complete dataset 33832162\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Load the complete dataset file\n",
    "complete_ratings_file = os.path.join(datasets_path, 'ml-latest', 'ratings.csv')\n",
    "complete_ratings_raw_data = sc.textFile(complete_ratings_file)\n",
    "complete_ratings_raw_data_header = complete_ratings_raw_data.take(1)[0]\n",
    "\n",
    "# Parse\n",
    "complete_ratings_data = complete_ratings_raw_data.filter(lambda line: line!=complete_ratings_raw_data_header)\\\n",
    "    .map(lambda line: line.split(\",\")).map(lambda tokens: (int(tokens[0]),int(tokens[1]),float(tokens[2]))).cache()\n",
    "    \n",
    "print (\"There are %s recommendations in the complete dataset\", format(complete_ratings_data.count()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0805626",
   "metadata": {},
   "source": [
    "Now we are ready to train the recommender model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33a959a5",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "training_RDD, test_RDD = complete_ratings_data.randomSplit([7, 3], seed=0L)\n",
    "\n",
    "complete_model = ALS.train(training_RDD, best_rank, seed=seed, \n",
    "                           iterations=iterations, lambda_=regularization_parameter)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9e2976c",
   "metadata": {},
   "source": [
    "Now we test on our testing set.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a81076c4",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For testing data the RMSE is 0.82183583368\n"
     ]
    }
   ],
   "source": [
    "test_for_predict_RDD = test_RDD.map(lambda x: (x[0], x[1]))\n",
    "\n",
    "predictions = complete_model.predictAll(test_for_predict_RDD).map(lambda r: ((r[0], r[1]), r[2]))\n",
    "rates_and_preds = test_RDD.map(lambda r: ((int(r[0]), int(r[1])), float(r[2]))).join(predictions)\n",
    "error = math.sqrt(rates_and_preds.map(lambda r: (r[1][0] - r[1][1])**2).mean())\n",
    "    \n",
    "print 'For testing data the RMSE is %s' % (error)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d790839e",
   "metadata": {},
   "source": [
    "I got a more accurate recommender when using a much larger dataset.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d93a2d3d",
   "metadata": {},
   "source": [
    "## Make recommendations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa848138",
   "metadata": {},
   "source": [
    "Load the movies complete file for later use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "cdc1e1c3",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are %s movies in the complete dataset 86537\n"
     ]
    }
   ],
   "source": [
    "complete_movies_file = os.path.join(datasets_path, 'ml-latest', 'movies.csv')\n",
    "complete_movies_raw_data = sc.textFile(complete_movies_file)\n",
    "complete_movies_raw_data_header = complete_movies_raw_data.take(1)[0]\n",
    "\n",
    "# Parse\n",
    "complete_movies_data = complete_movies_raw_data.filter(lambda line: line!=complete_movies_raw_data_header)\\\n",
    "    .map(lambda line: line.split(\",\")).map(lambda tokens: (int(tokens[0]),tokens[1],tokens[2])).cache()\n",
    "\n",
    "complete_movies_titles = complete_movies_data.map(lambda x: (int(x[0]),x[1]))\n",
    "    \n",
    "print (\"There are %s movies in the complete dataset\", complete_movies_titles.count())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a20619b0",
   "metadata": {},
   "source": [
    "Count the number of ratings per movie.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "d0120427",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'complete_ratings_data' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[30], line 5\u001b[0m\n\u001b[1;32m      2\u001b[0m     nratings \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(ID_and_ratings_tuple[\u001b[38;5;241m1\u001b[39m])\n\u001b[1;32m      3\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m ID_and_ratings_tuple[\u001b[38;5;241m0\u001b[39m], (nratings, \u001b[38;5;28mfloat\u001b[39m(\u001b[38;5;28msum\u001b[39m(x \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m ID_and_ratings_tuple[\u001b[38;5;241m1\u001b[39m]))\u001b[38;5;241m/\u001b[39mnratings)\n\u001b[0;32m----> 5\u001b[0m movie_ID_with_ratings_RDD \u001b[38;5;241m=\u001b[39m (\u001b[43mcomplete_ratings_data\u001b[49m\u001b[38;5;241m.\u001b[39mmap(\u001b[38;5;28;01mlambda\u001b[39;00m x: (x[\u001b[38;5;241m1\u001b[39m], x[\u001b[38;5;241m2\u001b[39m]))\u001b[38;5;241m.\u001b[39mgroupByKey())\n\u001b[1;32m      6\u001b[0m movie_ID_with_avg_ratings_RDD \u001b[38;5;241m=\u001b[39m movie_ID_with_ratings_RDD\u001b[38;5;241m.\u001b[39mmap(get_counts_and_averages)\n\u001b[1;32m      7\u001b[0m movie_rating_counts_RDD \u001b[38;5;241m=\u001b[39m movie_ID_with_avg_ratings_RDD\u001b[38;5;241m.\u001b[39mmap(\u001b[38;5;28;01mlambda\u001b[39;00m x: (x[\u001b[38;5;241m0\u001b[39m], x[\u001b[38;5;241m1\u001b[39m][\u001b[38;5;241m0\u001b[39m]))\n",
      "\u001b[0;31mNameError\u001b[0m: name 'complete_ratings_data' is not defined"
     ]
    }
   ],
   "source": [
    "def get_counts_and_averages(ID_and_ratings_tuple):\n",
    "    nratings = len(ID_and_ratings_tuple[1])\n",
    "    return ID_and_ratings_tuple[0], (nratings, float(sum(x for x in ID_and_ratings_tuple[1]))/nratings)\n",
    "\n",
    "movie_ID_with_ratings_RDD = (complete_ratings_data.map(lambda x: (x[1], x[2])).groupByKey())\n",
    "movie_ID_with_avg_ratings_RDD = movie_ID_with_ratings_RDD.map(get_counts_and_averages)\n",
    "movie_rating_counts_RDD = movie_ID_with_avg_ratings_RDD.map(lambda x: (x[0], x[1][0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3cd0770",
   "metadata": {},
   "source": [
    "### Adding new user ratings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33697a2f",
   "metadata": {},
   "source": [
    "Now we need to rate some movies for the new user. We will put them in a new RDD and we will use the user ID 0, that is not assigned in the MovieLens dataset. Check the [dataset](http://grouplens.org/datasets/movielens/) movies file for ID to Tittle assignment (so you know what movies are you actually rating).   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51332aac",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New user ratings: [(0, 260, 9), (0, 1, 8), (0, 16, 7), (0, 25, 8), (0, 32, 9), (0, 335, 4), (0, 379, 3), (0, 296, 7), (0, 858, 10), (0, 50, 8)]\n"
     ]
    }
   ],
   "source": [
    "new_user_ID = 0\n",
    "\n",
    "# The format of each line is (userID, movieID, rating)\n",
    "new_user_ratings = [\n",
    "     (0,260,9), # Star Wars (1977)\n",
    "     (0,1,8), # Toy Story (1995)\n",
    "     (0,16,7), # Casino (1995)\n",
    "     (0,25,8), # Leaving Las Vegas (1995)\n",
    "     (0,32,9), # Twelve Monkeys (a.k.a. 12 Monkeys) (1995)\n",
    "     (0,335,4), # Flintstones, The (1994)\n",
    "     (0,379,3), # Timecop (1994)\n",
    "     (0,296,7), # Pulp Fiction (1994)\n",
    "     (0,858,10) , # Godfather, The (1972)\n",
    "     (0,50,8) # Usual Suspects, The (1995)\n",
    "    ]\n",
    "new_user_ratings_RDD = sc.parallelize(new_user_ratings)\n",
    "print 'New user ratings: %s' % new_user_ratings_RDD.take(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "853dde56",
   "metadata": {},
   "source": [
    "Now we add them to the data we will use to train our recommender model. We use Spark's `union()` transformation for this.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "3caffe9f",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'complete_ratings_data' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[31], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m complete_data_with_new_ratings_RDD \u001b[38;5;241m=\u001b[39m \u001b[43mcomplete_ratings_data\u001b[49m\u001b[38;5;241m.\u001b[39munion(new_user_ratings_RDD)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'complete_ratings_data' is not defined"
     ]
    }
   ],
   "source": [
    "complete_data_with_new_ratings_RDD = complete_ratings_data.union(new_user_ratings_RDD)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14035272",
   "metadata": {},
   "source": [
    "And finally we train the ALS model using all the parameters we selected before (when using the small dataset)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd9af9a2",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New model trained in 56.61 seconds\n"
     ]
    }
   ],
   "source": [
    "from time import time\n",
    "\n",
    "t0 = time()\n",
    "new_ratings_model = ALS.train(complete_data_with_new_ratings_RDD, best_rank, seed=seed, \n",
    "                              iterations=iterations, lambda_=regularization_parameter)\n",
    "tt = time() - t0\n",
    "\n",
    "print \"New model trained in %s seconds\" % round(tt,3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc518d8c",
   "metadata": {},
   "source": [
    "It took some time. We will need to repeat that every time a user add new ratings. Ideally we will do this in batches, and not for every single rating that comes into the system for every user."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "733cb068",
   "metadata": {},
   "source": [
    "### Getting top recommendations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe658d2e",
   "metadata": {},
   "source": [
    "Let's now get some recommendations! For that we will get an RDD with all the movies the new user hasn't rated yet. We will them together with the model to predict ratings.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d5068ed",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "new_user_ratings_ids = map(lambda x: x[1], new_user_ratings) # get just movie IDs\n",
    "# keep just those not on the ID list (thanks Lei Li for spotting the error!)\n",
    "new_user_unrated_movies_RDD = (complete_movies_data.filter(lambda x: x[0] not in new_user_ratings_ids).map(lambda x: (new_user_ID, x[0])))\n",
    "\n",
    "# Use the input RDD, new_user_unrated_movies_RDD, with new_ratings_model.predictAll() to predict new ratings for the movies\n",
    "new_user_recommendations_RDD = new_ratings_model.predictAll(new_user_unrated_movies_RDD)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc92b8b3",
   "metadata": {},
   "source": [
    "We have our recommendations ready. Now we can print out the 25 movies with the highest predicted ratings. And join them with the movies RDD to get the titles, and ratings count in order to get movies with a minimum number of counts. First we will do the join and see what does the result looks like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86d6fa0b",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(87040, ((6.834512984654888, u'\"Housemaid'), 14)),\n",
       " (8194, ((5.966704041954459, u'Baby Doll (1956)'), 79)),\n",
       " (130390, ((0.6922328127396398, u'Contract Killers (2009)'), 1))]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Transform new_user_recommendations_RDD into pairs of the form (Movie ID, Predicted Rating)\n",
    "new_user_recommendations_rating_RDD = new_user_recommendations_RDD.map(lambda x: (x.product, x.rating))\n",
    "new_user_recommendations_rating_title_and_count_RDD = \\\n",
    "    new_user_recommendations_rating_RDD.join(complete_movies_titles).join(movie_rating_counts_RDD)\n",
    "new_user_recommendations_rating_title_and_count_RDD.take(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bff438f7",
   "metadata": {},
   "source": [
    "So we need to flat this down a bit in order to have `(Title, Rating, Ratings Count)`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f06e6165",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "new_user_recommendations_rating_title_and_count_RDD = \\\n",
    "    new_user_recommendations_rating_title_and_count_RDD.map(lambda r: (r[1][0][1], r[1][0][0], r[1][1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15dba292",
   "metadata": {},
   "source": [
    "Finally, get the highest rated recommendations for the new user, filtering out movies with less than 25 ratings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51f6a08e",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TOP recommended movies (with more than 25 reviews):\n",
      "(u'\"Godfather: Part II', 8.503749129186701, 29198)\n",
      "(u'\"Civil War', 8.386497469089297, 257)\n",
      "(u'Frozen Planet (2011)', 8.372705479107108, 31)\n",
      "(u'\"Shawshank Redemption', 8.258510064442426, 67741)\n",
      "(u'Cosmos (1980)', 8.252254825768972, 948)\n",
      "(u'Band of Brothers (2001)', 8.225114960311624, 4450)\n",
      "(u'Generation Kill (2008)', 8.206487040524653, 52)\n",
      "(u\"Schindler's List (1993)\", 8.172761674773625, 53609)\n",
      "(u'Dr. Strangelove or: How I Learned to Stop Worrying and Love the Bomb (1964)', 8.166229786764168, 23915)\n",
      "(u\"One Flew Over the Cuckoo's Nest (1975)\", 8.15617022970577, 32948)\n",
      "(u'Casablanca (1942)', 8.141303207981174, 26114)\n",
      "(u'Seven Samurai (Shichinin no samurai) (1954)', 8.139633165142612, 11796)\n",
      "(u'Goodfellas (1990)', 8.12931139039048, 27123)\n",
      "(u'Star Wars: Episode V - The Empire Strikes Back (1980)', 8.124225700242096, 47710)\n",
      "(u'Jazz (2001)', 8.078538221315313, 25)\n",
      "(u\"Long Night's Journey Into Day (2000)\", 8.050176820606127, 34)\n",
      "(u'Lawrence of Arabia (1962)', 8.041331489948814, 13452)\n",
      "(u'Raiders of the Lost Ark (Indiana Jones and the Raiders of the Lost Ark) (1981)', 8.0399424815528, 45908)\n",
      "(u'12 Angry Men (1957)', 8.011389274280754, 13235)\n",
      "(u\"It's Such a Beautiful Day (2012)\", 8.007734839026181, 35)\n",
      "(u'Apocalypse Now (1979)', 8.005094327199552, 23905)\n",
      "(u'Paths of Glory (1957)', 7.999379786394267, 3598)\n",
      "(u'Rear Window (1954)', 7.9860865203540214, 17996)\n",
      "(u'State of Play (2003)', 7.981582126801772, 27)\n",
      "(u'Chinatown (1974)', 7.978673289692703, 16195)\n"
     ]
    }
   ],
   "source": [
    "top_movies = new_user_recommendations_rating_title_and_count_RDD.filter(lambda r: r[2]>=25).takeOrdered(25, key=lambda x: -x[1])\n",
    "\n",
    "print ('TOP recommended movies (with more than 25 reviews):\\n%s' %\n",
    "        '\\n'.join(map(str, top_movies)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4100da5f",
   "metadata": {},
   "source": [
    "### Getting individual ratings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7a5056a",
   "metadata": {},
   "source": [
    "Another useful usecase is getting the predicted rating for a particular movie for a given user. The process is similar to the previous retreival of top recommendations but, instead of using `predcitAll` with every single movie the user hasn't rated yet, we will just pass the method a single entry with the movie we want to predict the rating for.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ed496e4",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Rating(user=0, product=122880, rating=4.955831875971526)]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_movie = sc.parallelize([(0, 500)]) # Quiz Show (1994)\n",
    "individual_movie_rating_RDD = new_ratings_model.predictAll(new_user_unrated_movies_RDD)\n",
    "individual_movie_rating_RDD.take(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84a43e4e",
   "metadata": {},
   "source": [
    "Not very likely that the new user will like that one... Obviously we can include as many movies as we need in that list!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d83be3be",
   "metadata": {},
   "source": [
    "## Persisting the model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0af9c067",
   "metadata": {},
   "source": [
    "Optionally, we might want to persist the base model for later use in our on-line recommendations. Although a new model is generated everytime we have new user ratings, it might be worth it to store the current one, in order to save time when starting up the server, etc. We might also save time if we persist some of the RDDs we have generated, specially those that took longer to process. For example, the following lines save and load a ALS model.      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31f20865",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.mllib.recommendation import MatrixFactorizationModel\n",
    "\n",
    "model_path = os.path.join('..', 'models', 'movie_lens_als')\n",
    "\n",
    "# Save and load model\n",
    "model.save(sc, model_path)\n",
    "same_model = MatrixFactorizationModel.load(sc, model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0a16610",
   "metadata": {},
   "source": [
    "Among other things, you will see in your filesystem that there are folder with product and user data into [Parquet](https://parquet.apache.org/) format files.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "febb07f3",
   "metadata": {},
   "source": [
    "## Genre and other fields"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "199a333a",
   "metadata": {},
   "source": [
    "We havent used the `genre` and `timestamp` fields in order to simplify the transformations and the whole tutorial. Incorporating them doesn't reprensent any problem. A good use could be filtering recommendations by any of them (e.g. recommendations by genre, or recent recommendations) like we have done with the minimum number of ratings.  "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
